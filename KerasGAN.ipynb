{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple GAN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9643144250863969060\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1472135168\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18234107187098113454\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#check if tensorflow is using GPU\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available GPUs\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow for Keras backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Make sure we can reproduce the experiment and get the same result\n",
    "np.random.seed(10)\n",
    "\n",
    "# The dimension of our random noise vector\n",
    "random_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering and preprocessing MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    # load the data\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # normalize our inputs to be in the range[-1, 1]\n",
    "    # remember that the value of each attribute (pixel) ranges from 0 to 255\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
    "    # 784 columns per row\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our optimizer\n",
    "We will use the stochastic gradient-based optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "def get_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our generator and discriminator\n",
    "\n",
    "Each function will be represented by a Neural Network with three hidden layers. The generator will have fully connected layers (Dense) whereas the discriminator will use layers with a dropout rate of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "def get_generator(optimizer):\n",
    "    generator = Sequential()\n",
    "    \n",
    "    # first hidden layer\n",
    "    generator.add(Dense(256, input_dim=random_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # second hidden layer\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # third hidden layer\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # the output layer (784 units) has a different activation function\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    \n",
    "    # a Keras model needs two arguments to compile: loss function and optimizer\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# discriminator\n",
    "def get_discriminator(optimizer):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return discriminator\n",
    "\n",
    "# the number of units in the layers of the generator and discriminator\n",
    "# are symmetrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the\n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "    # get the output of the discriminator (probability if the image is real or not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wall of generated MNIST images\n",
    "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_image_epoch_{0}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "Train_on_batch runs a single gradient update for a batch of certain batch_size. Essentially, it runs the optimization algorithm (Adam, in this case) in a mini-batch manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # Get the training and testing data\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data()\n",
    "    # Split the training data into batches of size 128\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    "\n",
    "    # Build our GAN netowrk\n",
    "    adam = get_optimizer()\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch', e, '-'*15)\n",
    "        for _ in tqdm(range(batch_count)):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generated_images = generator.predict(noise)\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            # First half of the images are real, second half are fake\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            # One-sided label smoothing - why??\n",
    "            y_dis[:batch_size] = 0.9\n",
    "\n",
    "            # Train discriminator on batch\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train generator on batch\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            # The output is defined as 'real' (y = 1)\n",
    "            # We want to minimize the difference between the discriminator predictions\n",
    "            # on the generated images and the probability of them being real.\n",
    "            # That is, we want to train the GAN so that the generated images\n",
    "            # are more likely to be classified as real by the discriminator\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
